{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation Tutorial\n",
    "\n",
    "Welcome to the dataset preparation tutorial! In this notebook, we will download the toy data set for the tutorial and prepare the necessary tables used for later analysis. Here are the steps we will review:\n",
    "\n",
    "1. Verify prerequisites\n",
    "2. Create a new project workspace\n",
    "3. Review sample dataset\n",
    "4. Build the proxy table\n",
    "5. Run regional annotation ETL\n",
    "\n",
    "**NOTE**: All of the configuration files for this tutorial have been provided in the container. The host and port values in the configuration files are dynamically set based on your system. \n",
    "\n",
    "**NOTE**: The current working directory is '~/vmount/notebooks'. All file and directory paths specified in the configuration files are relative to the current working directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Verify prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the software prerequisites for executing tasks with luna packages. These prerequiristes have already been baked into this docker container. Too view the setup, please see the corresponding dockerfile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T14:11:02.591178Z",
     "iopub.status.busy": "2021-11-16T14:11:02.590864Z",
     "iopub.status.idle": "2021-11-16T14:11:05.749394Z",
     "shell.execute_reply": "2021-11-16T14:11:05.748361Z",
     "shell.execute_reply.started": "2021-11-16T14:11:02.591150Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 --version\n",
    "!java -version\n",
    "!echo LUNA_HOME: $LUNA_HOME\n",
    "!which jupyter\n",
    "!pip list | grep luna-\n",
    "import luna\n",
    "luna.__path__\n",
    "import luna.pathology\n",
    "luna.pathology.__path__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a new project workspace\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a luna home space and place the configuration files there. Using a manifest file, we will create a project workspace for your configurations, data, models, and outputs to go for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ~/luna\n",
    "cp -R ~/vmount/conf ~/luna\n",
    "cat ~/luna/conf/manifest.yaml\n",
    "python3 -m luna.project.generate --manifest_file ~/luna/conf/manifest.yaml\n",
    "tree ~/vmount/PRO_12-123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see a new directory called *PRO_12-123* with the manifest file in it. This will be your project workspace!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Review sample dataset\n",
    "\n",
    "The data that you will be using for this tutorial is a set of 5 whole slide images of ovarian cancer H&E slides, available in the svs file format. Whole slide imaging refers to the scanning of conventional glass slides for research purposes; in this case, these are slides that oncologists have used to inspecting cancer samples!\n",
    "\n",
    "While bringing up the DSA container, we already ran a script to get the data, and set up DSA. The `vmount/provision.py` script ran these steps:\n",
    "  \n",
    "  - Set up admin user and default assetstore\n",
    "  \n",
    "  - Download sample data from [public kitware site](https://data.kitware.com/#user/61b9f3dc4acac99f42ca7678/folder/61b9f4564acac99f42ca7692). to `~/vmount/PRO_12-123/data/toy_data_set/`\n",
    "  \n",
    "  - Create a collection and add slides/annotations to your local DSA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tree ~/vmount/PRO_12-123/data/toy_data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to import your own data, you can do so from your local filesystem as well as an object store. For more details, refer to the [girder user documentation](https://girder.readthedocs.io/en/latest/user-guide.html#assetstores)\n",
    "\n",
    "To import images from your local filesystem, \n",
    "\n",
    "- Login to DSA with admin/password\n",
    "- Add images to your local computer at `vmount/assetstore` \n",
    "- Navigate to **Admin Console** -> **Assetstores**\n",
    "- From the default assetstore, click on **Import data**\n",
    "- Specify the path to the images you wish to import. e.g. `/assetstore/yourimage` and click import\n",
    "\n",
    "As the `/assetstore` mount is available to DSA, this import should be much faster than uploading the image through the **Upload files** in the UI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the proxy table\n",
    "\n",
    "Now, we will run the Whole Slide Image (WSI) ETL to build a meta-data catalog of the slides in a proxy table. \n",
    "\n",
    "For reference, ETL stands for extract-transform-load; it is the method that often involves cleaning data, transforming data types, and loading data into different systems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!slide_etl ~/vmount/PRO_12-123/data/toy_data_set \\\n",
    "--project_name PRO_12-123 --comment \"Example Ingestion Job\" \\\n",
    "--store_url \"\" --no-write \\\n",
    "-o ../PRO_12-123/data/toy_data_set/table/SLIDES "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step may take a while. At the end, your proxy table should be generated!\n",
    "\n",
    "Before we view the table, we must first update it to associate patient ID's with the slides. This is necessary for correctly training and validating the machine learning model in the coming notebooks. Once the slides are divided into \"tiles\" in the next notebook, the tiles are split between the training and validation sets for the ML model. If the tiles do not have patient ID's associated with them, then it is possible for tiles from one individual to appear in both the training and validation of the model; this would cause researchers to have an exaggerated interpretation of the model's accuracy, since we would essentially be validating the model on information that is too near to what it has already seen. \n",
    "\n",
    "Note that we will not be using patient IDs associated with MSK. Instead, we will be using spoof IDs that will suffice for this tutorial. When running this workflow with real data, make sure to include the IDs safely and securely. Run the following block of code to add a 'patient_id' column to the table and store it using Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we may view the WSI table! This table should have the metadata associated with the WSI slides that you just collected, including the patient IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_parquet(\"../PRO_12-123/data/toy_data_set/table/SLIDES/slide_ingest_PRO_12-123.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the table is depicted above, congratulations, you  have successfully run the Whole Slide Image (WSI) ETL to database the slides!\n",
    "\n",
    "## Run the regional annotation ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole slide images that you downloaded are images of ovarian cancer, but not every pixel on each slide is a tumor. In fact, the images show tumor cells, normal ovarian cells and more. A non-expert annotated this slide for demo purposes only.\n",
    "\n",
    "The regional annotation ETL performs the following steps\n",
    "\n",
    "- Downloads DSA json annotations\n",
    "- Converts DSA jsons to GeoJSON format, which is compatible with downstream applications\n",
    "- Saves configs in your `~/vmount/PRO_12-123/configs/REGIONAL_METADATA_RESULTS`\n",
    "- Saves parquet table in your `~/vmount/PRO_12-123/tables/REGIONAL_METADATA_RESULTS `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the regional annotation ETL, we use the `dsa_annotation` CLI. For more details on the dsa_annotation, and the annotations we support, please checkout the `7_dsa-annotation.ipynb` notebook.\n",
    "\n",
    "**Note**: details of your DSA instance is specified as `DSA_URI` in `../conf/dsa_regional_annotation.yaml` and should be updated to reflect your DSA setup.  If you are using the docker, replace the `localhost` with the IP you get from running:\n",
    "\n",
    "```docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' luna_tutorial_girder_1```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!dsa_annotation http://pllimsksparky3:8081/api/v1 \\\n",
    "    --collection_name \"TCGA collection\" \\\n",
    "    --annotation_name \"ov_regional\" \\\n",
    "    --username admin --password password \\\n",
    "    --output_dir ../PRO_12-123/data/toy_data_set/table/ANNOTATIONS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that the regional annotation ETL was correctly run, after the Jupyter cell finishes, you may load the regional annotations table! This table contains the metadata saved from running the ETL. It includes paths to the bitmap files, numpy files, and geoJSON files that were mentioned before. To load the table, run the following code cell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyarrow.parquet import read_table\n",
    "\n",
    "pd.read_parquet(\"../PRO_12-123/data/toy_data_set/table/ANNOTATIONS/slide_annotation_dataset_TCGA collection_ov_regional.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, lets get our geojsons and join on slide id!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(\"../PRO_12-123/data/toy_data_set/table/ANNOTATIONS/slide_annotation_dataset_TCGA collection_ov_regional.parquet\") \\\n",
    "    .query(\"type=='geojson'\")[['slide_geojson']] \\\n",
    "    .join(\n",
    "        pd.read_parquet(\"../PRO_12-123/data/toy_data_set/table/SLIDES/slide_ingest_PRO_12-123.parquet\")['slide_image']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you have successfully set up your workspace, dowloaded the data, and run both the pathology and regional annotation ETLs to prepare your data. You are ready to move on to the tiling notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
